import torch
import torch.nn as nn
import torch.nn.functional as F
from .base_model import BaseModel
from models import get_model
from transformers import get_cosine_schedule_with_warmup

class Trainer(BaseModel):
    def name(self):
        return 'Trainer'

    def __init__(self, opt):
        super(Trainer, self).__init__(opt)
        self.opt = opt
        self.model = get_model(opt.arch, opt)
        self.lr = opt.lr
        
        # åˆå§‹åŒ–åˆ†ç±»å¤´å‚æ•°
        if hasattr(self.model, 'fc'):
            for m in self.model.fc.modules():
                if isinstance(m, torch.nn.Linear):
                    torch.nn.init.normal_(m.weight.data, 0.0, opt.init_gain)
                    if m.bias is not None:
                        torch.nn.init.constant_(m.bias.data, 0.0)
                elif isinstance(m, torch.nn.LayerNorm):
                    nn.init.constant_(m.bias, 0)
                    nn.init.constant_(m.weight, 1.0)

        # å‚æ•°å†»ç»“ç­–ç•¥
        if opt.fix_backbone:
            params = []
            for name, p in self.model.named_parameters():
                if 'fc.' in name: 
                    params.append(p)
                    p.requires_grad = True
                elif any(x in name for x in ['S_residual', 'U_residual', 'V_residual']):
                    params.append(p)
                    p.requires_grad = True
                else:
                    p.requires_grad = False
            print(f">>> Backbone fixed. Training {len(params)} tensors (Head + SVD Residuals).")
        else:
            print("Your backbone is not fixed. Training all parameters.")
            params = self.model.parameters()

        # ä¼˜åŒ–å™¨
        if opt.optim == 'adam':
            self.optimizer = torch.optim.AdamW(params, lr=opt.lr, betas=(opt.beta1, 0.999), weight_decay=opt.weight_decay)
        elif opt.optim == 'sgd':
            self.optimizer = torch.optim.SGD(params, lr=opt.lr, momentum=0.9, weight_decay=opt.weight_decay)
        else:
            raise ValueError("optim should be [adam, sgd]")

        # Loss å‡½æ•°é…ç½®
        self.loss_fn = nn.CrossEntropyLoss()
        self.margin = 5.0        
        self.lambda_ebm = 0.5    
        self.lambda_smooth = 0.1 

        self.model.to(opt.gpu_ids[0])
        
        self.scheduler = None
        # æ£€æŸ¥å‚æ•°æ˜¯å¦å­˜åœ¨ä¸”å¤§äº0
        if hasattr(opt, 'warmup_steps') and opt.warmup_steps > 0:
            # ä¼˜å…ˆä½¿ç”¨æˆ‘ä»¬åœ¨ train.py é‡Œç®—å¥½çš„å€¼
            # å¦‚æœæ²¡ç®—ï¼ˆä¸ºäº†å…¼å®¹æ—§ä»£ç ï¼‰ï¼Œå†å›é€€åˆ° opt.niter * 1000
            total_steps = getattr(opt, 'total_steps_for_scheduler', opt.niter * 1000)
            
            print(f">>> Initializing Scheduler: Warmup={opt.warmup_steps}, Total Steps={total_steps}")
            self.scheduler = get_cosine_schedule_with_warmup(
                self.optimizer, 
                num_warmup_steps=opt.warmup_steps, 
                num_training_steps=total_steps
            )
        else:
            print(">>> âš ï¸ Warning: Scheduler NOT initialized (warmup_steps is 0 or missing).")

    def set_input(self, input):
        self.input = input[0].to(self.device)
        self.label = input[1].to(self.device).long()

    def forward(self):
        # è®­ç»ƒè¿‡ç¨‹ç›´æ¥è°ƒç”¨ï¼Œæ¨¡å‹å†…éƒ¨æ ¹æ® self.training è¿”å› 5 å…ƒç»„
        self.output = self.model(self.input)
        
        # è§£æè¿”å›å€¼
        if isinstance(self.output, tuple) and len(self.output) == 5:
            self.logits = self.output[0]
            self.e_real = self.output[1]
            self.e_fake = self.output[2]
            self.e_real_noisy = self.output[3]
            self.e_fake_noisy = self.output[4]
            self.output = self.logits
        elif isinstance(self.output, tuple) and len(self.output) == 3:
            self.logits = self.output[0]
            self.e_real = self.output[1]
            self.e_fake = self.output[2]
            self.output = self.logits
        else:
            self.logits = self.output
            self.e_real = None

    def test(self):
        # ä¸“é—¨ç”¨äº test_diffusion.py çš„æµ‹è¯•æ–¹æ³•
        with torch.no_grad():
            self.output = self.model(self.input, return_energy=True)
            
            if isinstance(self.output, tuple) and len(self.output) >= 3:
                self.logits = self.output[0]
                self.e_real = self.output[1]
                self.e_fake = self.output[2]
                self.output = self.logits
            else:
                self.logits = self.output
    
    def get_loss(self):
            loss_cls = self.loss_fn(self.logits, self.label)
            
            if self.e_real is None:
                return loss_cls
    
            fake_mask = (self.label == 1)
            real_mask = (self.label == 0)
            
            loss_energy = 0.0
            
            # =========================================================
            # ğŸŒŸ ç­–ç•¥ï¼šä¸å¯¹ç§° Margin + L1 ç»å¯¹é”šå®š
            # =========================================================
            margin_real = 5.0   # ä¸¥
            margin_fake = 2.5   # å®½
            lambda_abs = 0.1    # é”šå®šæƒé‡
            
            # 1. å¯¹æ¯”æŸå¤±
            loss_contrast = 0.0
            if real_mask.sum() > 0:
                loss_contrast += F.relu(self.e_real[real_mask] - self.e_fake[real_mask] + margin_real).mean()
            if fake_mask.sum() > 0:
                loss_contrast += F.relu(self.e_fake[fake_mask] - self.e_real[fake_mask] + margin_fake).mean()
            
            # 2. ç»å¯¹èƒ½é‡çº¦æŸ (L1)
            loss_abs = 0.0
            if real_mask.sum() > 0:
                loss_abs = lambda_abs * torch.abs(self.e_real[real_mask]).mean()
                
            loss_energy = loss_contrast + loss_abs
    
            # =========================================================
    
            # Smoothness Loss
            loss_smooth = 0.0
            if hasattr(self, 'e_real_noisy') and self.e_real_noisy is not None:
                loss_smooth = F.mse_loss(self.e_real, self.e_real_noisy) + \
                              F.mse_loss(self.e_fake, self.e_fake_noisy)
            
            # ğŸ“Š ã€ç›‘æ§æ¨¡å—ã€‘(å…³é”®ä¿®å¤)
            # fix: ä½¿ç”¨ self.opt.isTrain æ›¿ä»£ self.training (Trainer æ²¡æœ‰è¯¥å±æ€§)
            if self.opt.isTrain and hasattr(self, 'total_steps') and self.total_steps % 100 == 0:
                 e_real_val = self.e_real[real_mask].mean().item() if real_mask.sum() > 0 else 0.0
                 e_fake_val = self.e_fake[fake_mask].mean().item() if fake_mask.sum() > 0 else 0.0
                 print(f" [Energy] Real: {e_real_val:.3f} | Fake: {e_fake_val:.3f} | Gap: {e_fake_val - e_real_val:.3f}")
    
            total_loss = loss_cls + self.lambda_ebm * loss_energy + self.lambda_smooth * loss_smooth
            return total_loss

    def optimize_parameters(self):
        self.model.train() 
        self.forward()
        self.loss = self.get_loss()
        self.optimizer.zero_grad()
        self.loss.backward()
        
        # ğŸ›¡ï¸ æ¢¯åº¦è£å‰ª (Gradient Clipping)
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 5.0) 
        
        self.optimizer.step()
        if self.scheduler:
            self.scheduler.step()
